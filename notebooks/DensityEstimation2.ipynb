{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Parametric Density Estimation and Clustering\n",
    "\n",
    "Gordon Richards (2016, 2018, 2020, 2022, 2024) based on materials from Vanderplas, Leighly, Thibert, and Ivezic 4.4, 6.3, 6.4, 6.6.  With updates to my own class from [Stephen Taylor's class at Vanderbilt](https://github.com/VanderbiltAstronomy/astr_8070_s24)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Contents\n",
    "* [Parametric density estimation](#one)\n",
    "* [Expectation Maximization](#two)\n",
    "* [Clustering](#three)\n",
    "* [Aside: Scalers](#four)\n",
    "* [Aside: Pipelines](#five)\n",
    "* [DBSCAN](#six)\n",
    "* [Hierarchical Clustering](#seven)\n",
    "* [Correlation Functions](#eight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Parametric Density Estimation <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "### Gaussian Mixture Models (GMM)\n",
    "\n",
    "KDE centers each bin (or rather, kernel) at each point.  In a [**mixture model**](https://en.wikipedia.org/wiki/Mixture_model) we don't use a kernel for each data point, but rather we fit for the ***locations of the kernels*** in addition to the width. So a mixture model is sort of a hybrid between a tradtional (fixed bin location/size) histogram and KDE. \n",
    "\n",
    "- Using lots of kernels (maybe even more than the BIC score suggests) may make sense if you just want to provide an accurate description of the data (as in density estimation).  \n",
    "- Using fewer kernels makes mixture models more like clustering (later today), where the suggestion is still to use many kernels in order to divide the sample into real clusters and \"background\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Gaussians are the most commonly used components for mixture models.  So, the pdf is modeled by a sum of Gaussians:\n",
    "$$p(x) = \\sum_{k=1}^N \\alpha_k \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $\\alpha_k$ are the \"mixing coefficients\" with $0\\le \\alpha_k \\le 1$ and $\\sum_{k=1}^N \\alpha_k = 1$.\n",
    "\n",
    "We can solve for the parameters using maximum likelihood analyis as we have discussed previously.\n",
    "However, this can be complicated in multiple dimensions, requiring the use of [**Expectation Maximization (EM)**](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) methods (see textbook for details).\n",
    "\n",
    "Ivezic Figure 4.2 (next cell) provides an example in 1-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 4.2\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the dataset.\n",
    "#  We'll create our dataset by drawing samples from Gaussians.\n",
    "\n",
    "random_state = np.random.RandomState(seed=1)\n",
    "\n",
    "X = np.concatenate([random_state.normal(-1, 1.5, 350),\n",
    "                    random_state.normal(0, 1, 500),\n",
    "                    random_state.normal(3, 0.5, 150)]).reshape(-1, 1)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Learn the best-fit GaussianMixture models\n",
    "#  Here we'll use scikit-learn's GaussianMixture model. The fit() method\n",
    "#  uses an Expectation-Maximization approach to find the best\n",
    "#  mixture of Gaussians for the data\n",
    "\n",
    "# fit models with 1-10 components\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)\n",
    "\n",
    "# compute the AIC and the BIC\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "#  We'll use three panels:\n",
    "#   1) data + best-fit mixture\n",
    "#   2) AIC and BIC vs number of components\n",
    "#   3) probability that a point came from each component\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3.4))\n",
    "fig.subplots_adjust(left=0.12, right=0.97,\n",
    "                    bottom=0.21, top=0.9, wspace=0.5)\n",
    "\n",
    "\n",
    "# plot 1: data + best-fit mixture\n",
    "ax = fig.add_subplot(131)\n",
    "M_best = models[np.argmin(AIC)]\n",
    "\n",
    "x = np.linspace(-6, 6, 1000)\n",
    "logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "ax.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)\n",
    "ax.plot(x, pdf, '-k')\n",
    "ax.plot(x, pdf_individual, '--k')\n",
    "ax.text(0.04, 0.96, \"Best-fit Mixture\",\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p(x)$')\n",
    "\n",
    "\n",
    "# plot 2: AIC and BIC\n",
    "ax = fig.add_subplot(132)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, '--k', label='BIC')\n",
    "ax.set_xlabel('n. components')\n",
    "ax.set_ylabel('information criterion')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "\n",
    "# plot 3: posterior probabilities for each component\n",
    "ax = fig.add_subplot(133)\n",
    "\n",
    "p = responsibilities\n",
    "p = p[:, (1, 0, 2)]  # rearrange order so the plot looks better\n",
    "p = p.cumsum(1).T\n",
    "\n",
    "ax.fill_between(x, 0, p[0], color='gray', alpha=0.3)\n",
    "ax.fill_between(x, p[0], p[1], color='gray', alpha=0.5)\n",
    "ax.fill_between(x, p[1], 1, color='gray', alpha=0.7)\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel(r'$p({\\rm class}|x)$')\n",
    "\n",
    "ax.text(-5, 0.3, 'class 1', rotation='vertical')\n",
    "ax.text(0, 0.5, 'class 2', rotation='vertical')\n",
    "ax.text(3, 0.3, 'class 3', rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Here we have some data that we are trying to describe with a mixture of between 1 and 10 Gaussians.  The figure shows an example where 3 Gaussians provides the best fit.\n",
    "\n",
    "Again, the distribution is modeled as \n",
    "\n",
    "$$p(x) = \\sum_{k=1}^N \\alpha_k \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "with $0\\le \\alpha_k \\le 1$ controlling the relative height of the Gaussians.  We use Expectation Maximization to determine the properties of each Gaussian ($\\alpha$, $\\mu_k$,and $\\Sigma_k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's try a simpler example so you can see exactly how to use GMM. We'll fit a two-dimensional distribution composed of two Gaussian distributions using GMMs with varying numbers of components. The BIC will tell us how many components are favored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.concatenate([np.random.normal(size=(500,2)),\n",
    "                   np.random.normal(loc=3.0, size=(500,2))]) # 500  points in 2D\n",
    "BIC = []\n",
    "for ii in range(1,6):\n",
    "    gmm = GaussianMixture(ii) # between 1 and 5 components\n",
    "    gmm.fit(X)\n",
    "    log_dens = gmm.score(X)\n",
    "    BIC.append(gmm.bic(X))\n",
    "    \n",
    "plt.plot(np.arange(1,6), BIC);\n",
    "plt.xlabel('Number of components');\n",
    "plt.ylabel('BIC Score');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Do the results make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Expectation Maximization (ultra simplified version)  <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "(Note: all explanations of EM are far more complicated than seems necessary for our purposes, so here is my overly simplified explanation.)\n",
    "\n",
    "This may make more sense in terms of our earlier Bayesian analyses if we write this as \n",
    "$$p(z=c) = \\alpha_k,$$\n",
    "and\n",
    "$$p(x|z=c) = \\mathscr{N}(x|\\mu_k,\\Sigma_k),$$\n",
    "where $z$ is a \"hidden\" variable related to which \"component\" each point is assigned to.\n",
    "\n",
    "In the Expectation step, we hold $\\mu_k, \\Sigma_k$, and $\\alpha_k$ fixed and compute the probability that each $x_i$ belongs to component, $c$.  \n",
    "\n",
    "In the Maximization step, we hold the probability of the components fixed and maximize $\\mu_k, \\Sigma_k,$ and $\\alpha_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Note that $\\alpha$ is the relative weight of each Gaussian component and not the probability of each point belonging to a specific component.  Can flip back to previous slide and show how a point can have probability of being in multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can use the following 2-D animation to illustrate the process.  \n",
    "\n",
    "We start with a 2-component GMM, where the initial components can be randomly determined.\n",
    "\n",
    "The points that are closest to the centroid of a component will be more probable under that distribution in the \"E\" step and will pull the centroid towards them in the \"M\" step.  Iteration between the \"E\" and \"M\" step eventually leads to convergence.\n",
    "\n",
    "In this particular example, 3 components better describes the data and similarly converges.  Note that the process is not that sensitive to how the components are first initialized.  We pretty much get the same result in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"B36fzChfyGU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A typical call to the [Gaussian Mixture Model](http://scikit-learn.org/stable/modules/mixture.html) algorithm looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.random.normal(size=(1000,2)) #1000  points in 2D\n",
    "gmm = GaussianMixture(3) #three components\n",
    "gmm.fit(X)\n",
    "log_dens = gmm.score(X)\n",
    "BIC = gmm.bic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's do the 1-D example given using eruption data from \"Old Faithful\" geyser at Yellowstone National Park.  \n",
    "[http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat](http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#eruptions: Eruption time in mins\n",
    "#waiting: Waiting time to next eruption\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/faithful.dat', delim_whitespace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Make two \"fancy\" histograms illustrating the distribution of `x=df['eruptions']` and `y=df['waiting']` times.  Use `bins=\"freedman\"` and `histtype=\"step\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as ____\n",
    "from astropy.visualization import hist as ____\n",
    "\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(121)\n",
    "fancyhist(___,___,___)\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('N')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "fancyhist(___,___,___)\n",
    "plt.xlabel('Waiting')\n",
    "plt.ylabel('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fit Gaussian Mixtures, first in 1-D\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#First fit Eruptions\n",
    "gmm1 = GaussianMixture(n_components=2) # 2-component gaussian mixture model\n",
    "gmm1.fit(df['eruptions'].values[:,None]) # Fit step\n",
    "xgrid1 = np.linspace(0, 8, 1000) # Make evaluation grid\n",
    "logprob1 = gmm1.score_samples(xgrid1[:,None]) # Compute log likelihoods on that grid\n",
    "pdf1 = np.exp(logprob1)\n",
    "resp1 = gmm1.predict_proba(xgrid1[:,None]) \n",
    "pdf_individual1 = resp1 * pdf1[:, np.newaxis] # Compute posterior probabilities for each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Then fit waiting\n",
    "gmm2 = GaussianMixture(n_components=___)\n",
    "gmm2.fit(___)\n",
    "xgrid2 = np.____(30, 120, 1000)\n",
    "logprob2 = gmm2.score_samples(___)\n",
    "pdf2 = np.exp(logprob2)\n",
    "resp2 = gmm2.____(___)\n",
    "pdf_individual2 = ___ * ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make plots\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "ax = fig.add_subplot(121)\n",
    "plt.hist(df['eruptions'], bins=6, density=True, histtype='step')\n",
    "plt.plot(xgrid1, pdf_individual1, '--', color='blue')\n",
    "plt.plot(xgrid1, pdf1, '-', color='gray')\n",
    "plt.xlabel(\"Eruptions\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "plt.hist(____, bins=9, ____, ___)\n",
    "plt.plot(___, ____, '--', color='blue')\n",
    "plt.plot(___, ____, '-', color='gray')\n",
    "plt.xlabel(\"Waiting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's now do a more complicated 1-D example (Ivezic, Figure 6.8), which compares a Mixture Model to KDE.\n",
    "[Note that the version at astroML.org has some bugs!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.8\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from astropy.visualization import hist\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "#  this is the same data used in the Bayesian Blocks figure\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "N = 10000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "true_pdf = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x)\n",
    "                          for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N), random_state=random_state)\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "random_state.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "fig.subplots_adjust(bottom=0.08, top=0.95, right=0.95, hspace=0.1)\n",
    "N_values = (500, 5000)\n",
    "subplots = (211, 212)\n",
    "k_values = (10, 100)\n",
    "\n",
    "for N, k, subplot in zip(N_values, k_values, subplots):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    xN = x[:N]\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "\n",
    "    # Compute density with KDE\n",
    "    kde = KernelDensity(bandwidth=0.1, kernel='gaussian')\n",
    "    kde.fit(xN[:, None])\n",
    "    dens_kde = np.exp(kde.score_samples(t[:, None]))\n",
    "\n",
    "    # Compute density via Gaussian Mixtures\n",
    "    # we'll try several numbers of clusters\n",
    "    n_components = np.arange(3, 16)\n",
    "    gmms = [GaussianMixture(n_components=n).fit(xN.reshape(-1, 1)) for n in n_components]\n",
    "    BICs = [gmm.bic(xN.reshape(-1, 1)) for gmm in gmms]\n",
    "    i_min = np.argmin(BICs)\n",
    "    t = np.linspace(-10, 30, 1000)\n",
    "    logprob = gmms[i_min].score_samples(t.reshape(-1, 1))\n",
    "\n",
    "    # plot the results\n",
    "    ax.plot(t, true_pdf(t), ':', color='black', zorder=3,\n",
    "            label=\"Generating Distribution\")\n",
    "    ax.plot(xN, -0.005 * np.ones(len(xN)), '|k', lw=1.5)\n",
    "    hist(xN, bins='blocks', ax=ax, density=True, zorder=1,\n",
    "         histtype='stepfilled', lw=1.5, color='k', alpha=0.2,\n",
    "         label=\"Bayesian Blocks\")\n",
    "    ax.plot(t, np.exp(logprob), '-', color='gray',\n",
    "            label=\"Mixture Model\\n(%i components)\" % n_components[i_min])\n",
    "    ax.plot(t, dens_kde, '-', color='black', zorder=3,\n",
    "            label=\"Kernel Density $(h=0.1)$\")\n",
    "\n",
    "    # label the plot\n",
    "    ax.text(0.02, 0.95, \"%i points\" % N, ha='left', va='top',\n",
    "            transform=ax.transAxes)\n",
    "    ax.set_ylabel('$p(x)$')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    if subplot == 212:\n",
    "        ax.set_xlabel('$x$')\n",
    "\n",
    "    ax.set_xlim(0, 20)\n",
    "    ax.set_ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's plot the BIC values and see why it picked that many components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.scatter(n_components,BICs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "What do the individual components look like?  Make a plot of those.  Careful with the shapes of the arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(gmms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See Ivezic, Figure 4.2 for help: http://www.astroml.org/book_figures/chapter4/fig_GMM_1D.html\n",
    "# The index \"8\" is choosing the instance with 11 components.\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "print(len(gmms[8].weights_))\n",
    "logprob =  gmms[8].score_samples(t[:,None])\n",
    "pdf = np.exp(logprob) # Sum of the individual component pdf\n",
    "resp = gmms[8].predict_proba(t[:,None]) # Array of \"responsibilities\" for each component\n",
    "pdf_individual = resp*pdf[:,None]\n",
    "plt.plot(t,pdf_individual)\n",
    "plt.xlim((0,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now let's look at the Old Faithful data again, but this time in 2-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(df['eruptions'],df['waiting'])\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('Waiting')\n",
    "plt.xlim([1.5,5.3])\n",
    "plt.ylim([40,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we'll fit both features at the same time (i.e., the $x$ and $y$ axes above).  Note that Scikit-Learn can handle Pandas DataFrames without further conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gmm3 = GaussianMixture(____=____)\n",
    "gmm3.fit(df[[____,____]]) #Note no need for \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Once the components have been fit, we can plot the location of the centroids and the \"error\" ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Kludge to fix the bug with draw_ellipse in astroML v1.0\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(mu, C, scales=[1, 2, 3], ax=None, **kwargs):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # find principal components and rotation angle of ellipse\n",
    "    sigma_x2 = C[0, 0]\n",
    "    sigma_y2 = C[1, 1]\n",
    "    sigma_xy = C[0, 1]\n",
    "\n",
    "    alpha = 0.5 * np.arctan2(2 * sigma_xy,\n",
    "                             (sigma_x2 - sigma_y2))\n",
    "    tmp1 = 0.5 * (sigma_x2 + sigma_y2)\n",
    "    tmp2 = np.sqrt(0.25 * (sigma_x2 - sigma_y2) ** 2 + sigma_xy ** 2)\n",
    "\n",
    "    sigma1 = np.sqrt(tmp1 + tmp2)\n",
    "    sigma2 = np.sqrt(tmp1 - tmp2)\n",
    "\n",
    "    for scale in scales:\n",
    "        ax.add_patch(Ellipse((mu[0], mu[1]),\n",
    "                             width=2 * scale * sigma1, height=2 * scale * sigma2,\n",
    "                             angle=alpha * 180. / np.pi,\n",
    "                             **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#See cell above\n",
    "#from astroML.plotting.tools import draw_ellipse\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.scatter(df['eruptions'],df['waiting'])\n",
    "plt.xlabel('Eruptions')\n",
    "plt.ylabel('Waiting')\n",
    "plt.xlim([1.5,5.3])\n",
    "plt.ylim([40,100])\n",
    "\n",
    "ax.scatter(gmm3.means_[:,0], gmm3.means_[:,1], marker='s', c='red', s=80)\n",
    "for mu, C, w in zip(gmm3.means_, gmm3.covariances_, gmm3.weights_):\n",
    "    draw_ellipse(mu, 2*C, scales=[1], ax=ax, fc='none', ec='k') #2 sigma ellipses for each component\n",
    "#Add a 3-sigma error ellipse \n",
    "for mu, C, w in zip(gmm3.means_, gmm3.covariances_, gmm3.weights_):\n",
    "    draw_ellipse(mu, 3*C, scales=[1], ax=ax, fc='none', ec='k') #3 sigma ellipses for each component\n",
    "#Add a 1-sigma error ellipse \n",
    "for mu, C, w in zip(gmm3.means_, gmm3.covariances_, gmm3.weights_):\n",
    "    draw_ellipse(mu, 1*C, scales=[1], ax=ax, fc='none', ec='k') #1 sigma ellipses for each component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Ivezic, Figure 6.6 shows another 2-D example.  In the first panel, we have the raw data.  We then try to represent the data with a series of Gaussians.  We allow up to 14 Gaussians and use the AIC/BIC to determine the best choice for this number.  This is shown in the second panel.  Finally, the third panel shows the chosen Gaussians with their centroids and 1-$\\sigma$ contours on top of a density plot of our data.\n",
    "\n",
    "In this case 5 components are required for the best fit.  While it looks like we could do a pretty good job with just 2 components, there does appear to be some \"background\" that is a high enough level to justify further components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Ivezic, Figure 6.6\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "from astroML.utils.decorators import pickle_results\n",
    "#See cells above\n",
    "#from astroML.plotting import draw_ellipse\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get the Segue Stellar Parameters Pipeline data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute GaussianMixture models & AIC/BIC\n",
    "N = np.arange(1, 14)\n",
    "\n",
    "\n",
    "@pickle_results(\"GMM_metallicity.pkl\")\n",
    "def compute_GaussianMixture(N, covariance_type='full', max_iter=1000):\n",
    "    models = [None for n in N]\n",
    "    for i in range(len(N)):\n",
    "        print(N[i])\n",
    "        models[i] = GaussianMixture(n_components=N[i], max_iter=max_iter,\n",
    "                                    covariance_type=covariance_type)\n",
    "        models[i].fit(X)\n",
    "    return models\n",
    "\n",
    "models = compute_GaussianMixture(N)\n",
    "\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]\n",
    "\n",
    "i_best = np.argmin(BIC)\n",
    "gmm_best = models[i_best]\n",
    "print(\"best fit converged:\", gmm_best.converged_)\n",
    "print(\"BIC: n_components =  %i\" % N[i_best])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute 2D density\n",
    "FeH_bins = 51\n",
    "alphFe_bins = 51\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'],\n",
    "                                          (FeH_bins, alphFe_bins))\n",
    "\n",
    "Xgrid = np.array(list(map(np.ravel,\n",
    "                          np.meshgrid(0.5 * (FeH_bins[:-1]\n",
    "                                             + FeH_bins[1:]),\n",
    "                                      0.5 * (alphFe_bins[:-1]\n",
    "                                             + alphFe_bins[1:]))))).T\n",
    "log_dens = gmm_best.score_samples(Xgrid).reshape((51, 51))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "fig.subplots_adjust(wspace=0.45,\n",
    "                    bottom=0.25, top=0.9,\n",
    "                    left=0.1, right=0.97)\n",
    "\n",
    "# plot density\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.text(0.93, 0.93, \"Input\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "# plot AIC/BIC\n",
    "ax = fig.add_subplot(132)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, ':k', label='BIC')\n",
    "ax.legend(loc=1)\n",
    "ax.set_xlabel('N components')\n",
    "plt.setp(ax.get_yticklabels(), fontsize=7)\n",
    "\n",
    "# plot best configurations for AIC and BIC\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(np.exp(log_dens),\n",
    "          origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "ax.scatter(gmm_best.means_[:, 0], gmm_best.means_[:, 1], c='w')\n",
    "for mu, C, w in zip(gmm_best.means_, gmm_best.covariances_, gmm_best.weights_):\n",
    "    draw_ellipse(mu, C, scales=[1.5], ax=ax, fc='none', ec='k')\n",
    "\n",
    "ax.text(0.93, 0.93, \"Converged\",\n",
    "        va='top', ha='right', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Talk about how to use this to do outlier finding.  Convolve with errors of unknown object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Lastly, let's look at a 2-D case where we are using GMM more to characterize the data than to find clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 6.7\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from astroML.datasets import fetch_great_wall\n",
    "from astroML.utils.decorators import pickle_results\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# load great wall data\n",
    "X = fetch_great_wall()\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create a function which will save the results to a pickle file\n",
    "#  for large number of clusters, computation will take a long time!\n",
    "@pickle_results('great_wall_GMM.pkl')\n",
    "def compute_GMM(n_clusters, max_iter=1000, tol=3, covariance_type='full'):\n",
    "    clf = GaussianMixture(n_clusters, covariance_type=covariance_type,\n",
    "                          max_iter=max_iter, tol=tol, random_state=0)\n",
    "    clf.fit(X)\n",
    "    print(\"converged:\", clf.converged_)\n",
    "    return clf\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a grid on which to evaluate the result\n",
    "Nx = 100\n",
    "Ny = 250\n",
    "xmin, xmax = (-375, -175)\n",
    "ymin, ymax = (-300, 200)\n",
    "\n",
    "#Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx), np.linspace(ymin, ymax, Ny)))).T\n",
    "Xgrid = np.vstack([np.ravel(arr) for arr in np.meshgrid(np.linspace(xmin, xmax, Nx), np.linspace(ymin, ymax, Ny))]) .T\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the results\n",
    "#\n",
    "# we'll use 100 clusters.  In practice, one should cross-validate\n",
    "# with AIC and BIC to settle on the correct number of clusters.\n",
    "clf = compute_GMM(n_clusters=100)\n",
    "log_dens = clf.score_samples(Xgrid).reshape(Ny, Nx)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 3.75))\n",
    "fig.subplots_adjust(hspace=0, left=0.08, right=0.95, bottom=0.13, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(211, aspect='equal')\n",
    "ax.scatter(X[:, 1], X[:, 0], s=1, lw=0, c='k')\n",
    "\n",
    "ax.set_xlim(ymin, ymax)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "plt.ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "ax = fig.add_subplot(212, aspect='equal')\n",
    "ax.imshow(np.exp(log_dens.T), origin='lower', cmap=plt.cm.binary,\n",
    "          extent=[ymin, ymax, xmin, xmax])\n",
    "ax.set_xlabel(r'$y\\ {\\rm (Mpc)}$')\n",
    "ax.set_ylabel(r'$x\\ {\\rm (Mpc)}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Note that this is very different than the non-parametric density estimates.  The advantage is that we now have a *model*.  This model can be stored very compactly with just a few numbers, unlike the KDE or KNN maps which require a floating point number for each grid point.  \n",
    "\n",
    "One thing that you might imagine doing with this is subtracting the model from the data and looking for interesting things among the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Clustering  <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "> [Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) algorithms attempt to group together like objects in a data set.  \n",
    "\n",
    "This process allows us to put new objects into the resulting classes and to identify rare objects that do not fit any particular mold.  **Clustering is inherently an \"unsupervised\" process** as we do not know the classification of the objects.  Since we have no metric for determining when we are right, it is a bit of a dark art, but it also can be very powerful.  Scikit-Learn's clustering suite is summarized at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### $K$-Means Clustering\n",
    "\n",
    "We start with [$K$-means clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), which is one of the simplest methods.  $K$-means seeks to minimize the following\n",
    "\n",
    "$$\\sum_{k=1}^{K}\\sum_{i\\in C_k}||x_i - \\mu_k||^2$$\n",
    "\n",
    "where $\\mu_k = \\frac{1}{N_k}\\sum_{i\\in C_k} x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This says to\n",
    "  * Take every object in class $C_k$ (as determined by which centroid it is closest to, specifically $C_k(x_i) = \\arg \\min_k ||x_i-\\mu_k||)$\n",
    "  * Compute the mean of the objects in that class\n",
    "  * Subtract that mean from each member of that class and square the norm\n",
    "  * Do that for each class and sum\n",
    "  * Shift the centroids of the *pre-determined* number of classes until this sum is minimized\n",
    "  * Do this multiple times with different starting centroids and take the result with the minimum sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A typical call will look something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = np.random.normal(size=(1000,2)) #1000 points in 2D\n",
    "clf = KMeans(n_clusters=3) #Try 3 clusters to start with\n",
    "clf.fit(X)\n",
    "centers=clf.cluster_centers_ #location of the clusters\n",
    "labels=clf.predict(X) #labels for each of the points\n",
    "\n",
    "# plot the data color-coded by cluster id\n",
    "colors = ['C0', 'C1', 'C2']\n",
    "for ii in range(3):\n",
    "    plt.scatter(X[labels==ii,0], X[labels==ii,1], color=colors[ii])\n",
    "    \n",
    "# To get some information on these try:\n",
    "# KMeans?\n",
    "# help(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Here is an example with the same data that we used for GMM.  Note how the background shifts the centroids from what you might expect.  So, the mixture model might work better in this case.\n",
    "\n",
    "However, one might consider running the K-means algorithm in order to find a suitable initialization for GMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 6.13\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from astroML.datasets import fetch_sdss_sspp\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data\n",
    "data = fetch_sdss_sspp(cleaned=True)\n",
    "X = np.vstack([data['FeH'], data['alphFe']]).T\n",
    "\n",
    "# truncate dataset for speed\n",
    "X = X[::5]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute a 2D histogram  of the input\n",
    "H, FeH_bins, alphFe_bins = np.histogram2d(data['FeH'], data['alphFe'], 50)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the KMeans clustering\n",
    "n_clusters = 4\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Visualize the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# plot density\n",
    "ax = plt.axes()\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[FeH_bins[0], FeH_bins[-1],\n",
    "                  alphFe_bins[0], alphFe_bins[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "FeH_centers = 0.5 * (FeH_bins[1:] + FeH_bins[:-1])\n",
    "alphFe_centers = 0.5 * (alphFe_bins[1:] + alphFe_bins[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(FeH_centers, alphFe_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "#H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "\n",
    "#for i in range(n_clusters):\n",
    "#    Hcp = H.copy()\n",
    "#    flag = (Hcp == i)\n",
    "#    Hcp[flag] = 1\n",
    "#    Hcp[~flag] = 0\n",
    "\n",
    "#    ax.contour(FeH_centers, alphFe_centers, Hcp, [-0.5, 0.5], linewidths=1, colors='k')\n",
    "\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(-1.101, 0.101)\n",
    "ax.set_ylim(alphFe_bins[0], alphFe_bins[-1])\n",
    "\n",
    "ax.set_xlabel(r'$\\rm [Fe/H]$')\n",
    "ax.set_ylabel(r'$\\rm [\\alpha/Fe]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "A few things to note\n",
    "* This was supposed to show the boundaries between the clusters, but it isn't working.  See another example below.\n",
    "* We scaled the data (subtracted the mean and scaling to unit variance) using `StandardScaler()` before running K-Means\n",
    "* We had to *un*scale the data to plot the centers\n",
    "* Plotting the cluster boundaries is not straightforward, but this gives you an example to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Aside: Scalers  <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Let's do an example with unscaled then scaled data so you can see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "#Make two blobs with 3 features\n",
    "X,y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X[:5,:],y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now make one of those features have a radically different scale\n",
    "X[:,0] = X[:,0]+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X[:5,:],y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make a scaled version of X (subtract the mean and divide by the standard deviation)\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_scaled[:5,:],y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot the unscaled and scaled data\n",
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=100, c=y)\n",
    "ax[1].scatter(X_scaled[:, 0], X_scaled[:, 1], s=100, c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Do KMeans clustering with 2 clusters on the scaled data.\n",
    "from sklearn.cluster import ____\n",
    "\n",
    "clf = KMeans(____=____) #Make 2 clusters to start with\n",
    "clf.fit(____)\n",
    "centers=clf.cluster_centers_ #location of the clusters\n",
    "labels=clf.predict(____) #labels for each of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot the unscaled and scaled data and the centers that we just computed\n",
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=100, c=labels)\n",
    "ax[0].scatter(centers[:, 0], centers[:, 1], s=150, c='red', edgecolors='k')\n",
    "\n",
    "ax[1].scatter(X_scaled[:, 0], X_scaled[:, 1], s=100, c=labels)\n",
    "ax[1].scatter(centers[:, 0], centers[:, 1], s=150, c='red', edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Because the centers were computed with the scaled data, they don't correspond to the original (unscaled) data points.  But we can do an inverse transform if we want to be able to plot the data in its natural units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Unscale the centers and try again\n",
    "centers_unscaled = scaler.inverse_transform(centers)\n",
    "\n",
    "#Plot the unscaled and scaled data\n",
    "fig,ax = plt.subplots(1,2,figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=100, c=labels)\n",
    "ax[0].scatter(centers_unscaled[:, 0], centers_unscaled[:, 1], s=150, c='red', edgecolors='k')\n",
    "\n",
    "ax[1].scatter(X_scaled[:, 0], X_scaled[:, 1], s=100, c=labels)\n",
    "ax[1].scatter(centers[:, 0], centers[:, 1], s=150, c='red', edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Aside: Pipelines <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "Note that because of the standardization enabled by the Scikit-Learn API, it is easy to chain operations together into a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).  See below for a very simple example of how to do the same thing that we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=2))])\n",
    "                 \n",
    "pipe.fit(X)\n",
    "centers=pipe['kmeans'].cluster_centers_ #location of the clusters\n",
    "labels=pipe.predict(X) #labels for each of the points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Lastly, K-means is great for idealized data (roughly spherical clusters of similar density), but it has some problems with more complicated situations.  The following situation may not be realistic, but it illustrates where we can run into problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create two samples that are not spherically symmetric and try Kmeans.\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "____ = KMeans(n_clusters=2) #Try 2 clusters as there are clearly 2 by eye\n",
    "clf.____(____)\n",
    "centers=clf.____ #location of the clusters\n",
    "labels=clf.____ #labels for each of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=100, c=labels)\n",
    "\n",
    "plt.xlim(-1.5, 2.5)\n",
    "plt.ylim(-1.0, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Not so great.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### DBSCAN  <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "Let's try another algorithm, [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.dbscan.html), which is \"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".  This isn't discussed in the Ivezic book, but the link will take you to the description given in Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
    "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "    core_mask[dbscan.core_sample_indices_] = True\n",
    "    anomalies_mask = dbscan.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    cores = dbscan.components_\n",
    "    anomalies = X[anomalies_mask]\n",
    "    non_cores = X[non_core_mask]\n",
    "    \n",
    "    plt.scatter(cores[:, 0], cores[:, 1],\n",
    "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])\n",
    "    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n",
    "                c=\"r\", marker=\"x\", s=100)\n",
    "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\".\")\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "    #plt.title(\"eps={:.2f}, min_samples={}\".format(dbscan.eps, dbscan.min_samples), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Note that DBSCAN is *very* sensitive to these parameters.  Here we'll just do trial and error.  Try a few values of `eps` between 0.05 and 0.2 and `min_samples` between 3 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=____, min_samples=____)\n",
    "dbscan.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_dbscan(dbscan, X, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Not only is DBSCAN telling you which cluster each object belongs to, it is also \"outliers\" (denoted by the red crosses), which is another important use of clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Here is an example from Prof. Cruz, showing K-means boundaries (which were not working in our example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute the next few cells\n",
    "from astropy.table import Table\n",
    "t = Table.read('../data/cruz_all_dist.dat', format=\"ascii\")\n",
    "\n",
    "# Just something that you should know that you can do\n",
    "t[::10000].show_in_notebook(display_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn these data into a properly formatted Scikit-Learn array\n",
    "X = np.vstack([ t['col2'], t['col3'], t['col4'], t['col5'] ]).T\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project onto 2 axes with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # 2 components\n",
    "pca.fit(X) # Do the fitting\n",
    "\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], marker=\".\", color='k', edgecolors='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the KMeans clustering\n",
    "n_clusters = 6\n",
    "scaler = preprocessing.StandardScaler()\n",
    "clf = KMeans(n_clusters)\n",
    "clf.fit(scaler.fit_transform(X_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make some plots\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Compute a 2D histogram  of the input\n",
    "H, xedges, yedges = np.histogram2d(X_reduced[:,0], X_reduced[:,1], 50)\n",
    "\n",
    "# plot density\n",
    "#ax = plt.axes()\n",
    "ax.imshow(H.T, origin='lower', interpolation='nearest', aspect='auto',\n",
    "          extent=[xedges[0], xedges[-1],\n",
    "                  yedges[0], yedges[-1]],\n",
    "          cmap=plt.cm.binary)\n",
    "\n",
    "# plot cluster centers\n",
    "cluster_centers = scaler.inverse_transform(clf.cluster_centers_)\n",
    "ax.scatter(cluster_centers[:, 0], cluster_centers[:, 1],\n",
    "           s=40, c='w', edgecolors='k')\n",
    "\n",
    "# plot cluster boundaries\n",
    "x_centers = 0.5 * (xedges[1:] + xedges[:-1])\n",
    "y_centers = 0.5 * (yedges[1:] + yedges[:-1])\n",
    "\n",
    "Xgrid = np.meshgrid(x_centers, y_centers)\n",
    "Xgrid = np.array(Xgrid).reshape((2, 50 * 50)).T\n",
    "\n",
    "H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    Hcp = H.copy()\n",
    "    flag = (Hcp == i)\n",
    "    Hcp[flag] = 1\n",
    "    Hcp[~flag] = 0\n",
    "\n",
    "    ax.contour(x_centers, y_centers, Hcp, [-0.5, 0.5],\n",
    "               linewidths=1, colors='k')\n",
    "\n",
    "    \n",
    "    H = clf.predict(scaler.transform(Xgrid)).reshape((50, 50))\n",
    "    \n",
    "#ax.xaxis.set_major_locator(plt.MultipleLocator(0.3))\n",
    "ax.set_xlim(xedges[0], xedges[-1])\n",
    "ax.set_ylim(yedges[0], yedges[-1])\n",
    "\n",
    "ax.set_xlabel('Eigenvalue 1')\n",
    "ax.set_ylabel('Eigenvalue 2')\n",
    "\n",
    "plt.savefig('cruz.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Hierarchical Clustering  <a class=\"anchor\" id=\"seven\"></a>\n",
    "\n",
    "In [Hierarchical Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), we don't specify the number of clusters ahead of time, we start with $N$ clusters representing each data point.  Then the most similar clusters are joined together, the process repeating until some threshhold is reached.  Actually the process can go in the other direction as well.  What results is called a **dendrogram**, an example of which is shown below.\n",
    "\n",
    "![](https://ramaanathan.github.io/PSU_STAT505_MVA/L14_Clustering/Lesson14_Clustering_files/figure-markdown_github/unnamed-chunk-1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Clusters are merged at each step according to which are \"nearest\" to each other---where the definition of nearest needs to be specified.  A typical choice results in what is called a **minimum spanning tree** (which can be quite slow for large data sets).  Some threshhold needs to be specified to tell the process where to stop (e.g., we are going to treat the green and red objects in the example above as separate clusters).  \n",
    "\n",
    "![Here is an illustration](https://www.statisticshowto.com/wp-content/uploads/2016/11/clustergram.png)\n",
    "\n",
    "\n",
    "Below is an example call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "X = np.random.random((1000,2))\n",
    "G = kneighbors_graph(X, n_neighbors=10, mode='distance')\n",
    "T = minimum_spanning_tree(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "OK, but that's all that the book give us.  There is nothing about what to do with `G` and `T`.  So, instead I'm going to show you a really cool example from a colleague.  In this example Nathalie Thibert is taking spectroscopic data of a certain sub-class of quasars.  She is then grouping the objects into \"like\" bins using a hierarchical clustering algorithm.  The code below is based on the [scipy implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) and takes us through both the analysis and visualization of the data.  It makes use of the [Python Data Analysis Library (pandas)](http://pandas.pydata.org/) and ['pickled'](https://docs.python.org/2/library/pickle.html) \n",
    "data, the latter of which we have not talked about.\n",
    "\n",
    "For another detailed example of hierarchical clustering, see [https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/balquasar_data_new.pkl\", 'rb') as f:\n",
    "    data=pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load code/thibert_cluster1.py\n",
    "## Hierarchical Clustering Example: BAL Quasar UV Spectra (pre-reduced)\n",
    "## Author: Nathalie C. M. Thibert (Saint Mary's University), modified from\n",
    "## \t   code by Mark Daley (Western University)\n",
    "##\n",
    "## Method: Agglomerative Hierarchical Clustering\n",
    "## Distance Metric: Complete Linkage\n",
    "## Data: 100 BAL Quasar UV Spectra over ~1400-1550 Ang (i.e., the C IV BAL) \n",
    "##\t Spectra are already in rest-frame, normalized to the local continuum \n",
    "## \t and emission lines, and resampled to a common wavelength grid. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "# import clustering algorithms from scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Import pickled BAL quasar data.\n",
    "#data = pd.DataFrame(pd.read_pickle('../data/balquasar_data.pkl')) # Should have 500 wavelength values and 100 spectra.\n",
    "data = pd.read_pickle('./data/balquasar_data_new.pkl') # Should have 500 wavelength values and 100 spectra\n",
    "\n",
    "# Over plot some example spectra\n",
    "wl = np.arange(1400.1,1549.8,0.3)\n",
    "spec0 = data.T.iloc[0] # You can change the index to see different spectra (choose 0,1,2,...,99).\n",
    "spec5 = data.T.iloc[5]\n",
    "spec7 = data.T.iloc[7]\n",
    "plt.figure() \n",
    "plt.plot(wl,spec5)\n",
    "plt.plot(wl,spec0)\n",
    "plt.plot(wl,spec7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load code/thibert_cluster2.py\n",
    "## Hierarchical Clustering Example: BAL Quasar UV Spectra (pre-reduced)\n",
    "## Author: Nathalie C. M. Thibert (Saint Mary's University), modified from\n",
    "## \t   code by Mark Daley (Western University)\n",
    "##\n",
    "## Method: Agglomerative Hierarchical Clustering\n",
    "## Distance Metric: Complete Linkage\n",
    "## Data: 100 BAL Quasar UV Spectra over ~1400-1550 Ang (i.e., the C IV BAL) \n",
    "##\t Spectra are already in rest-frame, normalized to the local continuum \n",
    "## \t and emission lines, and resampled to a common wavelength grid. \n",
    "\n",
    "\n",
    "# Compute Pearson correlation matrix for 100 spectra. \n",
    "# Each element is a pairwise comparison b/w two spectra.\n",
    "c = data.corr() # Should have 100 rows and 100 columns.\n",
    "\n",
    "# Compute absolute-valued Pearson distance matrix.\n",
    "dp = 1.0 - np.abs(c)\n",
    "\n",
    "# Compute Euclidean distance matrix for the first dendrogram\n",
    "de1 = squareform(pdist(dp,metric='euclidean')) \n",
    "\n",
    "# Do it again for the second dendrogram\n",
    "de2 = squareform(pdist(dp.T,metric='euclidean'))\n",
    "\n",
    "# Start the dendrogram plot.\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Add the first dendrogram (on the left side)\n",
    "ax1 = f.add_axes([0.09, 0.1, 0.2, 0.6])\n",
    "Y = linkage(de1, method='complete') # This is where the hierarchical clustering takes place.\n",
    "Z1 = dendrogram(Y, orientation='left',show_leaf_counts=False, no_labels=True) # Plots dendrogram.\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# Add the second dendrogram (on the top)\n",
    "ax2 = f.add_axes([0.3, 0.71, 0.6, 0.2])\n",
    "Y = linkage(de2, method='complete')\n",
    "Z2 = dendrogram(Y,show_leaf_counts=False, no_labels=True)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "# Add the (main) plot of the (clustered) Euclidean distance matrix.\n",
    "axmatrix = f.add_axes([0.3, 0.1, 0.6, 0.6])\n",
    "idx1 = Z1['leaves']\n",
    "idx2 = Z2['leaves']\n",
    "D = de1[idx1, :]\n",
    "D = D[:, idx2]\n",
    "im = axmatrix.matshow(D, aspect='auto', origin='lower', cmap='hot')\n",
    "axmatrix.set_xticks([])\n",
    "axmatrix.set_yticks([])\n",
    "    \n",
    "axcolor = f.add_axes([0.91,0.1,0.02,0.6])\n",
    "pylab.colorbar(im,cax=axcolor)\n",
    "f.show()\n",
    "\n",
    "## NOTE: The colours in the dendrograms correspond to a flat clustering given \n",
    "##\t the default distance threshold in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Note that the side and top dendrograms are the same data.  It is just that the 2-D visualization better lets us see what groups go together.\n",
    "\n",
    "I don't pretend to fully understand each step of this process, but the end result is really cool and I think that there is enough here to get you started if we were interested in trying to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary of Chapter 6 methods from Ivezic Table 6.1 \n",
    "\n",
    "|Method          |Accuracy|Interpretability|Simplicity|Speed|\n",
    "|----------------|--------|----------------|----------|-----|\n",
    "|K-nearest Neighbor| H | H | H | M |\n",
    "|Kernel Density Estimation| H | H | H | H |\n",
    "|Gaussian Mixture Models| H | M | M | M |\n",
    "|Extreme Deconvolution| H | H | M | M |\n",
    "||||||\n",
    "|K-Means| L | M | H | M |\n",
    "|Max-radius minimization| L | M | M | M |\n",
    "|Mean shift| M | H | H | M |\n",
    "|Hierarchical Clustering| H | L | L | L |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Correlation functions <a class=\"anchor\" id=\"eight\"></a>\n",
    "\n",
    "(See Lecture 14 from Astro 8070 at Vanderbilt)\n",
    "\n",
    "> ***Correlation functions*** *tell us how far (and on what scales) a distribution of data samples differs from a random distribution.*\n",
    "\n",
    "They have been used extensively in astrophysics, e.g., \n",
    "- examining fluctuations and structure on varying scales of the galaxy density distribution in terms of luminosity, galaxy type, age of the Universe.\n",
    "- examining the two-point correlation function of temperature fluctuations of the cosmic microwave background to unveil the composition of the Universe.\n",
    "- searching for long-timescale correlations in time-series data to examine noise in AGN lightcurves or find GW signals in pulsar-timing data.\n",
    "\n",
    "\n",
    "![](https://www.astroml.org/_images/fig_corr_diagram_1.png)\n",
    "\n",
    "\n",
    "One of the most prominent is the **two-point correlation function** which characterizes the excess probability of finding pairs of points at varying separations when compared to a random distribution. It can be described in terms of the power spectrum of fluctuations, $P(k)$ where $k=2\\pi/\\lambda$ and $\\lambda$ is the scale/wavelength of the fluctuation:\n",
    "\n",
    "$$ \\xi(r) = \\frac{1}{2\\pi^2}\\int dk\\, k^2 P(k)\\frac{\\sin(kr)}{kr}.$$\n",
    "\n",
    "This correlation function can be used to describe the density fluctuations of sources by\n",
    "\n",
    "$$ \\xi(r) = \\left\\langle \\frac{\\delta\\rho(x)}{\\rho}\\frac{\\delta\\rho(x+r)}{\\rho}\\right\\rangle,$$\n",
    "\n",
    "where $\\delta\\rho(x)/\\rho = (\\rho-\\bar\\rho)/\\rho$ is the density contrast relative to the mean $\\bar\\rho$ at position $x$.\n",
    "\n",
    "In many situations in astronomy or cosmology, the spatial correlation function or angular correlation function is modeled as a power-law, e.g. $w(\\theta) = (\\theta/\\theta_0)^\\delta$ (this is astronomy after all, where power-laws rule supreme). Angular correlation functions are often used because we care about *projected structure* on different scales, rather than depth clustering, e.g., in the CMB this kind of angular clustering is indicative of fluctuations in the primordial density field that can unveil the composition of the Universe at the time of last scattering.\n",
    "\n",
    "Higher $n$-point correlation functions can be computed (see the image above), but the two-point function is the most common. Why? When dealing with large populations of sources spread across the Universe, we can often invoke the central limit theorem to describe the statistical distribution of sources-- hence **correlations can be approximated as obeying Gaussian statistics, which are entirely defined by the mean and two-point correlators (i.e. the variance)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this cell to implement AstroML's two-point correlation function on some random data\n",
    "from astroML.correlation import two_point_angular\n",
    "\n",
    "RA = 40 * np.random.random(1000)\n",
    "DEC = 10 * np.random.random(1000) # ra and dec in degrees\n",
    "\n",
    "bins = np.linspace(0.1, 10.0, 11) # edges for the 10 bins to evaluate\n",
    "corr = two_point_angular(RA, DEC, bins, method='landy-szalay')\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Now try the example from Figure 6.17 of the textbook. This computes the angular correlation function for a subset of the SDDS spectroscopic galaxy sample in the range $0.08<z<0.12$. This may take a minute or so to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from astroML.utils.decorators import pickle_results\n",
    "from astroML.datasets import fetch_sdss_specgals\n",
    "from astroML.correlation import bootstrap_two_point_angular\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data and do some quality cuts\n",
    "data = fetch_sdss_specgals()\n",
    "m_max = 17.7\n",
    "\n",
    "# redshift and magnitude cuts\n",
    "data = data[data['z'] > 0.08]\n",
    "data = data[data['z'] < 0.12]\n",
    "data = data[data['petroMag_r'] < m_max]\n",
    "\n",
    "# RA/DEC cuts\n",
    "RAmin, RAmax = 140, 220\n",
    "DECmin, DECmax = 5, 45\n",
    "data = data[data['ra'] < RAmax]\n",
    "data = data[data['ra'] > RAmin]\n",
    "data = data[data['dec'] < DECmax]\n",
    "data = data[data['dec'] > DECmin]\n",
    "\n",
    "ur = data['modelMag_u'] - data['modelMag_r']\n",
    "flag_red = (ur > 2.22)\n",
    "flag_blue = ~flag_red\n",
    "\n",
    "data_red = data[flag_red]\n",
    "data_blue = data[flag_blue]\n",
    "\n",
    "print(\"data size:\")\n",
    "print(\"  red gals: \", len(data_red))\n",
    "print(\"  blue gals:\", len(data_blue))\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Set up correlation function computation\n",
    "#  This calculation takes a long time with the bootstrap resampling,\n",
    "#  so we'll save the results.\n",
    "@pickle_results(\"correlation_functions.pkl\")\n",
    "def compute_results(Nbins=16, Nbootstraps=10,  method='landy-szalay', rseed=0):\n",
    "    np.random.seed(rseed)\n",
    "    bins = 10 ** np.linspace(np.log10(1 / 60.), np.log10(6), 16)\n",
    "\n",
    "    results = [bins]\n",
    "    for D in [data_red, data_blue]:\n",
    "        results += bootstrap_two_point_angular(D['ra'],\n",
    "                                               D['dec'],\n",
    "                                               bins=bins,\n",
    "                                               method=method,\n",
    "                                               Nbootstraps=Nbootstraps)\n",
    "\n",
    "    return results\n",
    "\n",
    "(bins, r_corr, r_corr_err, r_bootstraps,\n",
    " b_corr, b_corr_err, b_bootstraps) = compute_results()\n",
    "\n",
    "bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "corr = [r_corr, b_corr]\n",
    "corr_err = [r_corr_err, b_corr_err]\n",
    "bootstraps = [r_bootstraps, b_bootstraps]\n",
    "labels = ['$u-r > 2.22$\\n$N=%i$' % len(data_red),\n",
    "          '$u-r < 2.22$\\n$N=%i$' % len(data_blue)]\n",
    "\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "fig.subplots_adjust(bottom=0.2, top=0.9,\n",
    "                    left=0.13, right=0.95)\n",
    "\n",
    "for i in range(2):\n",
    "    ax = fig.add_subplot(121 + i, xscale='log', yscale='log')\n",
    "\n",
    "    ax.errorbar(bin_centers, corr[i], corr_err[i],\n",
    "                fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "    t = np.array([0.01, 10])\n",
    "    ax.plot(t, 10 * (t / 0.01) ** -0.8, ':k', linewidth=1)\n",
    "\n",
    "    ax.text(0.95, 0.95, labels[i],\n",
    "            ha='right', va='top', transform=ax.transAxes)\n",
    "    ax.set_xlabel(r'$\\theta\\ (deg)$')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r'$\\hat{w}(\\theta)$')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The left panel shows red galaxies with $u-r>2.22$ and the right panel shows blue galaxies with $u-r<2.22$. Uncertainties are derived from $10$ bootstrap samplings. There is significantly greater correlation structure on small angular scales in red galaxies than in blue galaxies. \n",
    "\n",
    "With tens of thousands of galaxies in each sample, this code has been optimized using **Scikit-Learn's ball-tree method for computing the correlation function-- further details given in Chapter 2, Section 2.52**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Final Projects"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
