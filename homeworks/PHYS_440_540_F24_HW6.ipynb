{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6: Problems\n",
    "## Due Wednesday 6 November, by 9:30am\n",
    "\n",
    "### PHYS 440/540, Fall 2024\n",
    "https://github.com/gtrichards/PHYS_440_540/\n",
    "\n",
    "\n",
    "## Problems 1\n",
    "\n",
    "Complete Chapter 4 of the *dimensionality reduction* course in Data Camp.\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "Do 1 more Data Camp chapters of your choice from within the following:\n",
    "* Interactive Data Visualization with Bokeh\n",
    "* Dimensionality reduction\n",
    "* Introduction to Data Visualization in Python\n",
    "* Introduction to Data Visualization with Matplotlib\n",
    "* Introduction to Data Visualization with Seaborn\n",
    "* Intermediate Data Visualization with Seaborn\n",
    "* Data Visualization for Everyone\n",
    "\n",
    "I'm not going to assign all of them as that will just cause confusion (in that you might think that I'm assigning 20 hours of work!).  So, you are just going to tell me which chapter you completed from among all the choices in this list of courses.  Do this by filling in the Course/Chapter title in the cell below.  Scroll through the course outlines first to see which ones look the most interesting/useful to you.\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "1. Read in a sample of SDSS imaging data. and create a mask to limit the data to \"good\" objects.  How many objects are left?  How many did you start with?\n",
    "\n",
    "2. Print out the feature names, and create a data matrix, X, out of a subset of features for the \"good\" objects.\n",
    "\n",
    "3. Use PCA (with randomized svd_solver for speed) to reduce the data matrix down to $2$ features. What is the explained variance of the data encapsulated in these eigen-features?\n",
    "\n",
    "4. Access the `type` key of the `data` structure and make an array of labels out of these. \n",
    "\n",
    "5. Make a scatter plot of the PCA-reduced data, colored by their corresponding `type`. *(You may want to set the transparency to be lower than 1 to see the mixing of samples.)*\n",
    "\n",
    "6. Choose $6000$ random integers between 0 and the number of samples in the data matrix. Record these integers because you'll use them later. \n",
    "\n",
    "7. Now try some non-linear dimensional reduction. These algorithms are slower than PCA, so you will operate only on the $6000$ random samples identified in the previous part. \n",
    "   - Try `LocallyLinearEmbedding`, `Isomap`, and `TSNE` algorithms, setting the number of components to be $2$ in all cases. \n",
    "\n",
    "8. As in the PCA case, make scatter plots of the dimensionally-reduced data, color coded by their `type`. For LLE and Isomap, experiment with the number of nearest neighbors between $5$ and $100$ to see what visually gives the best separation in `type` populations. For TSNE, do the same for the perplexity attribute. \n",
    "\n",
    "9. Which algorithm gives the cleanest way to visually see the two populations of sources? *(This will be subjective according to the samples you trained on, and even the randomness of the algorithms.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapters completed for Problems 2:\n",
    "* 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 3 Starter Code\n",
    "\n",
    "1. Read in a sample of SDSS imaging data. and create a mask to limit the data to \"good\" objects.  How many objects are left?  How many did you start with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from astroML.datasets import fetch_imaging_sample\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ____  \n",
    "print(data.shape)  # number of objects in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((data['iRaw']<21.0)&(data['uErr']<0.2)&(data['gErr']<0.2)&(data['rErr']<0.2)&(data['iErr']<0.2)&(data['zErr']<0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_good = data[____]\n",
    "print(len(____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out the feature names, and create a data matrix, X, out of a subset of features for the \"good\" objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keylist = ['rExtSFD', 'uRaw', 'gRaw', 'rRaw', 'iRaw', 'zRaw', \\\n",
    "           'uErr', 'gErr', 'rErr', 'iErr', 'zErr', \\\n",
    "           'uRawPSF', 'gRawPSF', 'rRawPSF', 'iRawPSF', 'zRawPSF', \n",
    "           'upsfErr', 'gpsfErr', 'rpsfErr', 'ipsfErr', 'zpsfErr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([data_good[key] for ____ in ____]) \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use PCA (with randomized svd_solver for speed) to reduce the data matrix down to $2$ features. What is the explained variance of the data encapsulated in these eigen-features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Randomized PCA for speed\n",
    "pca = PCA(____, svd_solver='____')\n",
    "X_reduced = pca.____(____)\n",
    "evals = pca.____\n",
    "print('Expained fractional variance of data encapsulated in the eigenvalues: ' + ____)\n",
    "print('Total explained variance: ' + ____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Access the `type` key of the `data` structure and make an array of labels out of these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(data_good[____]))\n",
    "#Note one of these types refers to stars, the other to galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a scatter plot of the PCA-reduced data, colored by their corresponding `type`. *(You may want to set the transparency to be lower than 1 to see the mixing of samples.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(____, ____, c=data_good[____], alpha=____)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Choose $6000$ random integers between 0 and the number of samples in the data matrix. Record these integers because you'll use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.____(low=____,high=____,size=____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now try some non-linear dimensional reduction. These algorithms are slower than PCA, so you will operate only on the $6000$ random samples identified in the previous part. \n",
    "   - Try `LocallyLinearEmbedding`, `Isomap`, and `TSNE` algorithms, setting the number of components to be $2$ in all cases. \n",
    "   \n",
    "7. As in the PCA case, make scatter plots of the dimensionally-reduced data, color coded by their `type`. For LLE and Isomap, experiment with the number of nearest neighbors between $5$ and $100$ to see what visually gives the best separation in `type` populations. For TSNE, do the same for the perplexity attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import ____\n",
    "k = ____ # Number of neighbors to use in fit\n",
    "n = ____ # Number of dimensions to fit\n",
    "\n",
    "lle = ____(____,____)\n",
    "\n",
    "X_reduced = ____.____(X[ind,:])\n",
    "\n",
    "plt.scatter(____,____,____,____)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import ____\n",
    "k = ____\n",
    "n = ____\n",
    "\n",
    "iso = ____(____,____)\n",
    "\n",
    "%etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import ____\n",
    "n = ____\n",
    "\n",
    "tsne = ____(____,perplexity=____)\n",
    "\n",
    "%etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8. Which algorithm gives the cleanest way to visually see the two populations of sources? *(This will be subjective according to the samples you trained on, and even the randomness of the algorithms.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any reasonable answer."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
