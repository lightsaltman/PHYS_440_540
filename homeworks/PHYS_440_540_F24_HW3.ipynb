{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Problems\n",
    "## Due Wednesday 16 October, by 9:30am\n",
    "\n",
    "### PHYS 440/540, Fall 2024\n",
    "https://github.com/gtrichards/PHYS_440_540/\n",
    "\n",
    "\n",
    "## Problem 1\n",
    "On Data Camp:\n",
    "\n",
    "Statistical Thinking in Python (Part 2): Bootstrap confidence intervals \n",
    "\n",
    "This is Chapter 2.  You shouldn't need Chapter 1, but you are welcome to do it.  If this chapter interests you, then you might get something out of completing Chapters 3 and 4 as well.\n",
    "\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "We're going to do some polynomial fits to data just like in the lecture.  But we didn't have time for the example in class, so please watch the Pre-Lecture video for Lecture 6 starting at 13:02 (until the end at 26:29).\n",
    "\n",
    "Specifically, let's use the example from http://jakevdp.github.io/blog/2015/08/07/frequentism-and-bayesianism-5-model-selection/ to illustrate some ideas about goodness of fit and model selection.\n",
    "\n",
    "Complete the cells in the starter code below to investigate how we can do both linear and polynomial fitting to two-dimensional data (y vs. x) and how to decide which of those models is \"best\".\n",
    "\n",
    "Comment on which of the models has the lowest BIC.  How does that compare to our chi-square results?  What do we learn from the difference?\n",
    " \n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Copy over all the code cells from our interactive MCMC example in Inference2 (7 cells in all from the cell that imports numpy to the histogram of the chain).  \n",
    "\n",
    "Change the code so that the likelihood plot is drawn in purple instead of green or red when the odds ratio for the next model is not favored, but is still larger than the random value used for the \"accept\" threshold.  So, now you will have green plots for steps that are both better and accepted, purple for steps that aren't better but are still accepted, and red for steps that are not accepted.  Also change the code so that the step size is small enough that you are likely to take a long time to reach the most likely value.\n",
    "\n",
    "Once you have done both of those things, then:\n",
    "\n",
    "* Run enough steps to show 2 steps that were accepted, even though they are worse.\n",
    "* Plot the chain showing it taking some time to get from the initial value to oscillating around the most likely value.\n",
    "* Plot a histogram of the chain both using the full chain and after throwing away the \"burn\" period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this cell to load all of the modules we'll need and define the data array.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate (x,y, sigma_y) \"data\" \n",
    "data = np.array([[ 0.42,  0.72,  0.  ,  0.3 ,  0.15,\n",
    "                   0.09,  0.19,  0.35,  0.4 ,  0.54,\n",
    "                   0.42,  0.69,  0.2 ,  0.88,  0.03,\n",
    "                   0.67,  0.42,  0.56,  0.14,  0.2  ],\n",
    "                 [ 0.33,  0.41, -0.25,  0.01, -0.05,\n",
    "                  -0.05, -0.12,  0.26,  0.29,  0.39, \n",
    "                   0.31,  0.42, -0.01,  0.58, -0.2 ,\n",
    "                   0.52,  0.15,  0.32, -0.13, -0.09 ],\n",
    "                 [ 0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
    "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1  ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to do a polynomial fit, compute the likelihood, and determine the best-fit parameters.\n",
    "\n",
    "#I mostly just want you to run this cell, but see if you can understand what is going on.\n",
    "#Add TWO lines of comments showing that you at least sort of understand.\n",
    "\n",
    "def polynomial_fit(theta, x):\n",
    "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
    "    # For a polynomial with order 1, this gives theta_0 + theta_1*x\n",
    "    # For a polynomial with order 2, this gives theta_0 + theta_1*x + theta_2*x^2, etc.\n",
    "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
    "\n",
    "# compute the data log-likelihood given a model\n",
    "def logL(theta, data, model=polynomial_fit):\n",
    "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
    "    x, y, sigma_y = data\n",
    "    y_fit = model(theta, x)\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(y, y_fit, sigma_y))\n",
    "\n",
    "# a direct optimization approach is used to get best model \n",
    "# parameters (which minimize -logL)\n",
    "def best_theta(degree, model=polynomial_fit, data=data):\n",
    "    theta_0 = (degree + 1) * [0]\n",
    "    neg_logL = lambda theta: -logL(theta, data, model)\n",
    "    return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this cell.\n",
    "\n",
    "#Again, I mostly just want you to run this cell, but see if you can understand what is going on.\n",
    "#Add TWO lines of comments showing that you at least sort of understand.\n",
    "\n",
    "x, y, sigma_y = data\n",
    "Ndata = x.size\n",
    "\n",
    "# get best-fit parameters for linear, quadratic and cubic models\n",
    "theta1 = best_theta(1, data=data)\n",
    "theta2 = best_theta(2, data=data)\n",
    "theta3 = best_theta(3, data=data)\n",
    "# generate best fit lines on a fine grid \n",
    "xgrid = np.linspace(-0.1, 1.1, 1000)\n",
    "yfit1 = polynomial_fit(theta1, xgrid)\n",
    "yfit2 = polynomial_fit(theta2, xgrid)\n",
    "yfit3 = polynomial_fit(theta3, xgrid)\n",
    "\n",
    "# plot \n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
    "ax.plot(xgrid, yfit1, label='best linear model')\n",
    "ax.plot(xgrid, yfit2, label='best quadratic model')\n",
    "ax.plot(xgrid, yfit3, label='best cubic model')\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set(xlabel='x', ylabel='y', title='data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete and execute this cell to compute chi2: sum{[(y-yfit)/sigma_y]^2} \n",
    "chi21 = np.sum(((y-polynomial_fit(theta1, x))/sigma_y)**2) \n",
    "chi22 = np.sum(((y-polynomial_fit(____, x))/sigma_y)**2) \n",
    "chi23 = np.sum(((y-polynomial_fit(____, x))/sigma_y)**2) \n",
    "# normalize by the number of degrees of freedom\n",
    "# the number of fitted parameters is 2, 3, 4\n",
    "chi2dof1 = chi21/(Ndata - 2)\n",
    "chi2dof2 = chi22/(____)\n",
    "chi2dof3 = chi23/(____)\n",
    "\n",
    "print(\"CHI2:\")\n",
    "print('   best linear model:', chi21)\n",
    "print('best quadratic model:', chi22)\n",
    "print('    best cubic model:', chi23)\n",
    "print(\"CHI2 per degree of freedom:\")\n",
    "print('   best linear model:', chi2dof1)\n",
    "print('best quadratic model:', chi2dof2)\n",
    "print('    best cubic model:', chi2dof3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete and execute this cell\n",
    "L = [] #Array with all of the likelihood values\n",
    "A = [] #Array with all of the AIC values\n",
    "B = [] #Array with all of the  BICvalues\n",
    "neg_LogL1 = -logL(theta1, data)+10  #+10 for plotting purposes\n",
    "neg_LogL2 = -logL(theta2, data)+10\n",
    "neg_LogL3 = -logL(theta3, data)+10\n",
    "L = np.append(L,neg_LogL1)\n",
    "L = np.append(L,neg_LogL2)\n",
    "L = np.append(L,neg_LogL3)\n",
    "\n",
    "xplot = np.arange(len(L))+1 #x values for plotting.\n",
    "\n",
    "AIC1 = 2.*neg_LogL1 + 2.*len(theta1) + 2.*len(theta1)*(len(theta1)+1)/(len(data[0])-len(theta1)-1)\n",
    "AIC2 = ____ #Complete\n",
    "AIC3 = ____ #Complete\n",
    "A = np.append(A,AIC1)\n",
    "A = np.append(A,AIC2)\n",
    "A = np.append(A,AIC3)\n",
    "BIC1 = 2.*neg_LogL1 + len(theta1)*np.log(len(data[0]))\n",
    "BIC2 = ____ #Complete\n",
    "BIC3 = ____ #Complete\n",
    "B = np.append(B,BIC1)\n",
    "B = np.append(B,BIC2)\n",
    "B = np.append(B,BIC3)\n",
    "\n",
    "print(\"-logL:\", L)\n",
    "print(\"AIC:\", A)\n",
    "print(\"BIC:\", B)\n",
    "\n",
    "\n",
    "fig = plt.subplots(figsize=(16, 8))\n",
    "plt.plot(xplot, L, label='Likelihood+10')\n",
    "plt.plot(xplot, A, label='AIC')\n",
    "plt.plot(xplot, B, label='BIC')\n",
    "plt.legend()\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Score\")\n",
    "print(L)\n",
    "print(B)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the 5 cells from lecture that are between these and modify according to the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "burn = 3000\n",
    "ax.hist(posterior,bins=30,alpha=0.5,density='True',label='estimated posterior')\n",
    "ax.hist(posterior[burn:],bins=30,alpha=0.5,density='True',label='estimated posterior no burn')\n",
    "xplot = np.linspace(-.5, .5, 500)\n",
    "post = calc_posterior_analytical(data, xplot, 0, 1)\n",
    "ax.plot(xplot, post, 'g', label='analytic posterior')\n",
    "_ = ax.set(xlabel='mu', ylabel='belief');\n",
    "ax.legend(fontsize=10);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
